{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empirical-flood",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import numpy as np\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "# create the context\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-survey",
   "metadata": {},
   "outputs": [],
   "source": [
    "songsdf = spark.read.parquet('s3://17700project/song_features.parquet')\n",
    "playcountdf = spark.read.parquet('s3://17700project/play_counts.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-suicide",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(songsdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-brand",
   "metadata": {},
   "outputs": [],
   "source": [
    "def song_track_str(x):\n",
    "    return x[2:-1]\n",
    "\n",
    "def split(x):\n",
    "    return x[2:-1].lower().split(' ')\n",
    "\n",
    "song_track_str_udf = udf(lambda x: song_track_str(x), StringType())\n",
    "split_udf = udf(lambda x: split(x), ArrayType(StringType()))\n",
    "\n",
    "songsdf_filtered = songsdf.select(song_track_str_udf('song_id').alias('song_id'), song_track_str_udf('track_id').alias('track_id'), \n",
    "                                  'duration', 'end_of_fade_in', 'key', 'key_confidence', 'loudness', 'mode', 'mode_confidence',\n",
    "                                  'start_of_fade_out', 'tempo', 'time_signature', 'time_signature_confidence', 'artist_familiarity', \n",
    "                                  'artist_hotttnesss', 'song_hotttnesss', split_udf('artist_name').alias('artist_name'), 'year', \n",
    "                                  split_udf('release').alias('release'), split_udf('title').alias('title'))\n",
    "\n",
    "playcountdf_filtered = playcountdf.select('songId', 'userId', 'Plays')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-thomson",
   "metadata": {},
   "outputs": [],
   "source": [
    "songsdf_filtered = songsdf_filtered.na.fill(value = 0, subset = ['artist_familiarity', 'artist_hotttnesss', 'song_hotttnesss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-improvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec, OneHotEncoder, StringIndexer\n",
    "\n",
    "#year\n",
    "stringIndexer = StringIndexer(inputCol=\"year\", outputCol=\"year_idx\")\n",
    "model = stringIndexer.fit(songsdf_filtered)\n",
    "songsdf_filtered = model.transform(songsdf_filtered)\n",
    "encoder = OneHotEncoder(inputCol=\"year_idx\", outputCol=\"year_enc\")\n",
    "songsdf_filtered = encoder.transform(songsdf_filtered)\n",
    "\n",
    "songsdf_filtered.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-verse",
   "metadata": {},
   "outputs": [],
   "source": [
    "#release\n",
    "word2Vec = Word2Vec(vectorSize=30, minCount=0, inputCol=\"release\", outputCol=\"release_enc\")\n",
    "model = word2Vec.fit(songsdf_filtered)\n",
    "songsdf_filtered = model.transform(songsdf_filtered)\n",
    "\n",
    "#title\n",
    "word2Vec = Word2Vec(vectorSize=30, minCount=0, inputCol=\"title\", outputCol=\"title_enc\")\n",
    "model = word2Vec.fit(songsdf_filtered)\n",
    "songsdf_filtered = model.transform(songsdf_filtered)\n",
    "\n",
    "#artist_name\n",
    "word2Vec = Word2Vec(vectorSize=30, minCount=0, inputCol=\"artist_name\", outputCol=\"artist_name_enc\")\n",
    "model = word2Vec.fit(songsdf_filtered)\n",
    "songsdf_filtered = model.transform(songsdf_filtered)\n",
    "\n",
    "songsdf_filtered.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-transsexual",
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresdf = songsdf_filtered.select('song_id', 'track_id', 'duration', 'end_of_fade_in', 'key', 'key_confidence', 'loudness', 'mode', \n",
    "                                     'mode_confidence', 'start_of_fade_out', 'tempo', 'time_signature', 'time_signature_confidence', \n",
    "                                     'artist_familiarity', 'artist_hotttnesss', 'song_hotttnesss', 'artist_name_enc', 'year_enc', \n",
    "                                     'release_enc', 'title_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-processor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['duration', 'end_of_fade_in', 'key', 'key_confidence', 'loudness', 'mode', 'mode_confidence', \n",
    "               'start_of_fade_out', 'tempo', 'time_signature', 'time_signature_confidence', 'artist_familiarity', 'artist_hotttnesss',\n",
    "               'song_hotttnesss', 'artist_name_enc', 'year_enc', 'release_enc', 'title_enc'],\n",
    "    outputCol='features')\n",
    "\n",
    "featuresdf = assembler.transform(featuresdf).select('song_id','track_id','features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-arizona",
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresdf.rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-median",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "pca = PCA(k=50, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(featuresdf)\n",
    "featuresdf = model.transform(featuresdf).select(\"song_id\",\"track_id\",col(\"pcaFeatures\").alias('features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-affiliate",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 180000\n",
    "(split_80_df, split_20_df) = playcountdf.randomSplit([0.8, 0.2], seed = seed)\n",
    "\n",
    "# Let's cache these datasets for performance\n",
    "training_df = split_80_df.cache()\n",
    "test_df = split_20_df.cache()\n",
    "\n",
    "print('Training: {0}, test: {1}\\n'.format(\n",
    "  training_df.count(), test_df.count())\n",
    ")\n",
    "training_df.show(3)\n",
    "test_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-retention",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(x,y):\n",
    "    return np.dot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-spain",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df_1 = training_df.join(featuresdf, training_df.songId == featuresdf.song_id).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-pharmacology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(songId, userId, similarity_metric): \n",
    "    song_features = featuresdf.filter(featuresdf.song_id == songId).rdd.take(1)[0].features\n",
    "    user_history_features = training_df_1.filter(training_df.userId == userId).rdd.map(lambda x: (x.features, x.Plays))\n",
    "    \n",
    "    user_history_similarity = user_history_features.map(lambda x: (similarity_metric(song_features, x[0]), x[1]))\n",
    "    numerator = user_history_similarity.map(lambda x: x[0]*x[1]).reduce(lambda a,b: a+b)\n",
    "    denominator = user_history_similarity.map(lambda x: x[0]).reduce(lambda a,b: a+b)\n",
    "    \n",
    "    return numerator/denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-technician",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict_udf = udf(lambda song_id, user_id: predict(song_id, user_id, dot_product), DoubleType())\n",
    "test_df_list = test_df.rdd.map(lambda x: (x.songId, x.userId, x.Plays)).take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earned-robinson",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "s = time.time()\n",
    "predict(test_sample[0], test_sample[1], dot_product)\n",
    "e = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-hebrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "s-e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "pred = []\n",
    "for i in test_df_list:\n",
    "    pred.append(predict(i[0], i[1], dot_product))\n",
    "e = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-words",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-cooperative",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
